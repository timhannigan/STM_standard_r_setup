---
title: 'IDeaS Standard Notebook for STM Rendering'
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

# Structural Topic Modeling - STM Rendering - Template Notebook IDeaS

### Maintained by the IDeaS Group 2022

This notebook assumes that some basic pre-processing has been done already. The required input is a csv file entitled "corpus_processed_for_STM.csv". This is from a working paper by Gorgi Krlev, Tim Hannigan, and Andre Spicer (2022). This file contains abstracts (and meta-data) from papers published at the Academy of Management Annals and International Journal of Management Reviews between 2009 and 2021.

The design of this notebook is to serve as a standard template to start a rendering process from. This code was inspired by Lindstedt (2019) and work by Tim Hannigan.

### Set up libraries

This notebook is presuming these libraries have been installed.

```{r}

# First, ensure that the RStudio environment is cleared so we're starting from scratch
rm(list = ls())

# setup code
knitr::opts_knit$set(root.dir = "../../")

# Structural topic modeling package 
library(stm)

# Network analysis and visualization package 
library(igraph)

# ggplot2 for visualizations
library(ggplot2)

# stminsights for interactive browsing of stm model
library(stminsights)

# helpful library
library(tidytext)

```

### PREPROCESSING

This notebook assumes some basic pre-processing has been done already. We'll attempt to use the standard STM functions to ensure that we can effectively use the covariates in the model.

We'll attempt to use the standard stm functions to ensure that we can effectively use the covariates in the model. These procedures will covert to lower case, remove punctuation, remove stopwords, remove numbers, and strip html.

### Load Corpus as CSV

```{r}
# Read csv formatted data 
data <- read.csv("Data/corpus_processed_for_STM.csv") 

```

### Fixing up corpus to match analysis conventions

To make our analysis easier, we're going to make some small changes to names. First, for the covariates we're going use, we'll make them all lowercase. Second, we'll rename *abstracts_processed* to *documents*. This is so we can keep our stm code slightly more generic and reusable across projects.

```{r}
# easier to deal with as a variable name
data$year <- data$Year
data$title <- data$Title
data$abstract <- data$Abstract

# copy this to a new column that is more generically named, easier for re-use
data$documents <- data$abstracts_processed

```

### Bringing our data into the stm library

We need to bring our data (corpus) into the stm library. There are some basic pre-processing functions built into stm, but instead of blackboxing these, we'll already done some of this work in our other notebook ("Rendering_corpus").

```{r}
# Process data using function textProcessor() 
processed <- textProcessor(data$documents, metadata = data, stem = FALSE, striphtml= TRUE)

# Prepare data using function prepDocuments() 
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 25)


```

### MODEL SELECTION

Recall the guidance on model selection in STM.

From Lindstedt (2019:311): "For shorter, focused corpora (i.e., those ranging from a few hundred to a few thousand documents in size), an initial choice between five and 50 topics is best, whereas for larger, unfocused corpora (i.e., those ranging from tens of thousands to hundreds of thousands of documents in size or larger), previous research has found that between 60 and 100 topics are best (Roberts et al. 2018)."

This is also where we set which covariates we want in the model. Recall that the values for the covariates are setup in the source dataframe ahead of time (before using this notebook in R).

```{r}
set.seed(02138)
start.time <- Sys.time()
set.seed(02138)
K_ <- c(10,15,20,25,30,35,40,45,50) # set the range here

# also note the number of cores you have available on your machine and set it here as a parameter cores=10

kResult <- searchK(out$documents, out$vocab, K = K_, prevalence =~ year + journal_code, init.type = "Spectral", data = out$meta, verbose=FALSE, cores=27)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken # this will tell you how long this procedure took to run
save(kResult, file = "Output/kResult.RData") # save results

```

#### Plot diagnostic results using built-in STM function, then save as PDF file

One way we can try to find some statistical evidence pointing to the optimal number of topics to use is to look for common plateaus across these diagnostics.

```{r}
set.seed(02138)
# first, save to file
pdf(file ="Output/kResult.pdf", width=10, height=8.5)
print(plot(kResult, cex=.25)) # note: importance of wrapping it here, so PDF file isn't corrupted
dev.off()

# then replot for on-screen use in RStudio
plot(kResult) 
```

From this, it seems that 30, 40 and 45 might be good candidates. However, an increasingly common practice in the IDeaS community (and the social sciences) is looking at how two particular metrics are jointly maximized (semantic-coherence and exclusivity).

#### Plot Semantic coherence-exclusivity plot using function plot()

This is where we're looking for a model specification that corresponds with the coherence-exclusivity frontier. Essentially, we're looking for a model that jointly maximizes Exclusivity and Semantic Coherence.

From Lindstedt (2019: 311):

"A secondary, less subjective, recommendation involves examining diagnostic tables and plots of semantic coherence and exclusivity calculations. Semantic coherence is a measure of the probability for a set of topic words to cooccur within the same document. Exclusivity is a measure of the probability for a word to fall primarily within the top rankings of a single topic. Model selection for the number of topics is made along the semantic coherence exclusivity "frontier" where no model is dominated by either metric (Roberts et al. 2014)."

However, the ultimate decision is comes down to human validation and interpretation.

"Given the difficulties associated with model selection and the trade-off between predictive and interpretative models, the ultimate responsibility for model selection rests with the researcher and their informed judgment. Therefore, it is on the researcher to "validate, validate, validate" their results (Grimmer and Stewart 2013:5). This process can be done in a number of ways, but the most useful means of validation in the stm R package is its built-in function that provides a list of the most representative documents for a particular topic."

```{r}
set.seed(02138)
pdf(file ="Output/coherence-exclusivity.pdf", width=10, height=8.5)
plot(kResult$results$semcoh, kResult$results$exclus, xlab = "Semantic Coherence", ylab = "Exclusivity")
# Add labels to semantic coherence-exclusivity plot using function text() 
text(kResult$results$semcoh, kResult$results$exclus, labels = paste("K", kResult$results$K), pos = 1)
dev.off()

# then replot for on-screen use in RStudio
plot(kResult$results$semcoh, kResult$results$exclus, xlab = "Semantic Coherence", ylab = "Exclusivity")
# Add labels to semantic coherence-exclusivity plot using function text() 
text(kResult$results$semcoh, kResult$results$exclus, labels = paste("K", kResult$results$K), pos = 1)
```

Here, we see 30 and 45 topics are most promising candidates.

### Validating the optimal model using *stminsights*

Grimmer & Stewart (2013) remind us that automated methods can only get us so far. The ultimate decision about the optimal number of topics is determined by human interpretation. We can think about the work we've done so far as supplementing our interpretive abilities. Automated methods are not a replacement for our interpretations informed by domain-expertise.

A good way to figure out which topic model to use is a tool called *stminsights*. This enables us to interactively browse a set of promising specifications. Save different specifications that best approximate the frontier, then browse in *stminsights* to validate.

For example, in the section above we found that 30 and 45 topics are promising models. We're going to use this tool to compare them in *stminsights*.

```{r}

i <- 30 # this is where we set the topic specification
set.seed(02138)

model_30 <- stm(out$documents, out$vocab, prevalence =~ year + journal_code, K = i, max.em.its = 150, data = out$meta, init.type = "Spectral", verbose=FALSE)
est_30 <- estimateEffect(~year + journal_code, model_30, uncertainty = "None", metadata = out$meta) 

# repeat the above block of code for other promising candidate models
i <- 45 # this is where we set the topic specification
set.seed(02138)

model_45 <- stm(out$documents, out$vocab, prevalence =~ year + journal_code, K = i, max.em.its = 150, data = out$meta, init.type = "Spectral", verbose=FALSE)
est_45 <- estimateEffect(~year + journal_code, model_45, uncertainty = "None", metadata = out$meta) 


save.image("Output/stm_promising_models.RData")
```

### Run interactive application using stminsights

This can help you to interactively browse the topic model. Load this tool in a web browser and then find the file called "stm_promising_models.RData" in the Output directory of this RProject.

```{r}
#run_stminsights()

```

Based on the interpretive analysis, it seems that 45 is the optimal number of topics to use. Be sure to press the "stop" button in RStudio so we can return to this workflow (away from the web browser with *stminsights*).

Once we decide on the optimal model, save it to to file

```{r}
i <- 45 # you set this value to the optimal value you determined (above)
model <- model_45 # we'll just call this "model" for now on, to make things easier.
save(model, file = sprintf("Output/stm_fit_%s_topics.RData", i) )
```

## Further Inspection of our Topic Model (45 topics)

Now that we have a validated topic, we can use it to do a distant reading. Let's check out our model.

First, we need to assign labels to topics based on what we think they mean (refer to).

#### Labeling topics

This is being saved from a dataframe to a csv file. You can open this up in Excel for easier usage.

```{r}
set.seed(02138)
topics <- labelTopics(model, n=10) # setting the top 10 words 
topics <- data.frame("features" = t(topics$frex)) # using FREX, but can also use prob, lift
colnames(topics) <- paste("Topics", c(1:i))
write.csv(topics, sprintf("Output/rendering_artifacts/topic_labels_K%s.csv", i), row.names = TRUE) 

```

We can open this csv file in Excel and start labeling the topic meanings. Note: this will show the top 10 weighted words in each topic. As a first pass, it's a good idea to use these to assign labels. It also might be helpful to look at word clouds for each topic (sized proportionally by weight of word in topic).

### Plot a wordcloud for topic

```{r}
# say we want to look for topic 4

j <- 4

set.seed(02138)
cloud(model, topic=j, scale=c(2,.25))

pdf(sprintf("Output/rendering_artifacts/topic_artifacts/topic_%s_word_cloud.pdf", j), width=5, height=5)
cloud(model, topic=j, scale=c(2,.25))
dev.off()

# then re-plot for on-screen use in RStudio
cloud(model, topic=j, scale=c(2,.25))
```

This pretty clearly refers to "organizational learning". Notice that this code-block also saved a version of this figure in our "Output/rendering_artifacts" folder. It would also be helpful to look at prototypical documents for this topic

### Find prototype documents using function findThoughts(); we can show this with titles, but can also use the abstracts themselves.

(Note, this duplicates the functionality in stminsights)

```{r}
set.seed(02138)
j <- 4
findThoughts(model, texts = data$title, topics = c(j), n = 4)$docs[[1]] # for titles
#findThoughts(model, texts = data$abstract, topics = c(j), n = 4)$docs[[1]] # for abstracts

# can also plot this
thoughts_topic<-findThoughts(model, texts = data$title, topics = c(j), n = 4)$docs[[1]] # for titles
#thoughts_topic<-findThoughts(model, texts = data$abstract, topics = c(j), n = 4)$docs[[1]] # for abstracts
pdf(sprintf("Output/rendering_artifacts/topic_artifacts/topic_%s_sample_docs.pdf", j), width=10, height=8.5)
plotQuote(thoughts_topic, width = 225, text.cex = .5, maxwidth = 1500)
dev.off()
# then replot for on-screen use in RStudio
plotQuote(thoughts_topic, width = 225, text.cex = .5, maxwidth = 1500)

```


We can also just run the following code-block to generate word-clouds and sample titles as files for all of the topics in our topic model.

### Generating rendering artifacts for all topics (in a loop)

```{r}
for (j in 1:i){
  set.seed(02138)
  thoughts_topic<- findThoughts(model, texts = data$title,
                                n = 4, topics = c(j))$docs[[1]]
  pdf(sprintf("Output/rendering_artifacts/topic_artifacts/topic_%s_sample_docs.pdf", j), width=10, height=8.5)
  plotQuote(thoughts_topic, width = 225, text.cex = .5, maxwidth = 1500)
  dev.off()
  
  # project the words in a topic
  set.seed(02138)
  pdf(sprintf("Output/rendering_artifacts/topic_artifacts/topic_%s_word_cloud.pdf", j), width=5, height=5)
  cloud(model, topic=j, scale=c(2,.25))
  dev.off()

}
```


### Topic Proportions

This is a helpful chart for us to see how certain topics are more dominant than others.

```{r}
set.seed(02138)

pdf(file =sprintf("Output/rendering_artifacts/expected_topic_proportions_K%s.pdf", i), width=20, height=20)
print(plot(model, cex=.05)) # note: importance of wrapping it here, so PDF file isn't corrupted
dev.off()

# then replot for on-screen use in RStudio
plot(model, cex=.05)

```

If we want this graph to be a bit nicer, we can insert our labels and then re-run it. Set the topic names to your relevant specification (fill in the custom labels here).

To DO

```{r}
topic_names <- c("TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:", "TOPIC LABEL:")
```

### Prepare Topic correlations using function topicCorr()

We can also see how certain topics are more or less correlated with one another in documents. Note the cutoff threshold parameter here. See manual for more details.

```{r}
set.seed(02138)
mod.out.coor <- topicCorr(model, method = "simple" , cutoff = 0.01)
```

# Covariate effects using function estimateEffect()

```{r}
set.seed(02138)
#covariates_est <- estimateEffect(~ s(year), model, uncertainty = "None", metadata = out$meta) 
```

### Covariate effects summary using function summary()

Will print all topics. Can also set topic=i for more specific output.

```{r}
#summary(covariates_est) 
```

# Model Summary

```{r}
# Model summary using function summary() 
summary(model)
```

